# Non-Local N2V: Improving N2V networks for spatially correlated noise

This is an official PyTorch implementation of "Non-Local N2V: Improving N2V networks for spatially correlated noise" published in [ICIP 2025](https://2025.ieeeicip.org/).

**Authors**:  
Diego Martin, Edoardo Peretti, Giacomo Boracchi

## Abstract

*Blind Spot Networks (BSNs) are powerful deep denoisers that can be trained in a self-supervised way, namely without access to clean images. The training scheme of BSN-based methods builds upon the assumption that noise is pixel-wise independent and zero-mean. However, this assumption is not satisfied in many real-world scenarios, where the noise exhibits spatial correlation that degrades BSN denoising performance. Training deep neural networks under spatially correlated noise has attracted a lot of interest. However, several approaches require complex network architectures or resource-demanding training procedures, resulting in less practical solutions than simple BSNs. In this work, we present Non-Local N2V (NL-N2V), an extension of the mainstream Noise2Void (N2V) method. It is designed to address spatially correlated noise while maintaining the simplicity and the network architecture of N2V. Our approach introduces a novel masking strategy inspired by the Non-Local Self-Similarity prior that masks small regions around blind spots and replaces these with values obtained from non-local similar patches. Tests performed on real-world datasets corrupted by correlated noise, such as SIDD and DND, show that NL-N2V considerably outperforms the traditional N2V.*

Here is the link for the [paper](#TODO Insert the paper link).

---

## üìÅ Repository Structure

The project is organized as follows:

```
NL-N2V
‚îú‚îÄ‚îÄ configs
‚îú‚îÄ‚îÄ datasets
‚îÇ   ‚îú‚îÄ‚îÄ train
‚îÇ   ‚îî‚îÄ‚îÄ validation
‚îú‚îÄ‚îÄ output
‚îÇ   ‚îú‚îÄ‚îÄ (run_name...)
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ data_handlers
‚îÇ   ‚îú‚îÄ‚îÄ loss
‚îÇ   ‚îú‚îÄ‚îÄ masker
‚îÇ   ‚îú‚îÄ‚îÄ model
‚îÇ   ‚îú‚îÄ‚îÄ trainer
‚îÇ   ‚îî‚îÄ‚îÄ util
```

### Some tips:

The `datasets` and `output` folders are not strictly required to be placed in the project workspace. Their location in the filesystem can be specified in the configuration files.

For a cleaner project, I suggest splitting the `datasets` folder into train, validation and test sub-folders.

The `output` folder, after being specified in the configuration file, is automatically generated by the File Manager. 
It will be divided into sub-folders called with the same name of the running experiment (run_name).  
Each sub-folder will contain: 
- `best_ckpts`: folder with the best checkpoints for the used metrics (PSNR and SSIM calculated on the validation set if present).
- `checkpoint`: folder with the model's checkpoints (all or last 2 depending on configuration).
- `img`: folder containing validation images.
- `logs`: folder containing the log files for the run.
- `tboard`: folder containing the tensorboard file.

## üöÄ Installation

The recommended way to install this project is to clone this repository from GitHub.  
Here an example of the steps you can take to clone the repo:

1. Create a workspace folder 
2. Clone the repo
3. Create and activate a virtual environment (inside the repo or the workspace)
4. Install all required dependences

```
# Create workspace
mkdir myworkspace
cd myworkspace

# Clone the repo
git clone https://github.com/DiegoMartin-prog/NL-N2V.git
cd NL-N2V

# Create a virtual environment (either with conda or pip)
python -m venv venv
source venv/bin/activate    # Linux/macOS
venv\Scripts\activate       # Windows

# Install dependences
pip install ...
```

### Libraries requirements

The project has been built on the following libraries with their indicated versions:

- Python 3.10.12
- PyTorch 2.5.1
- CUDA 12.4
- numpy 2.2.0
- opencv-python 4.10.0.84
- scikit-image 1.14.1

## üèÉ How to run the code

Here I will briefly explain how to run the code either for training, testing or reproduing some experiments with pretrained model weights.  
Note that we reproduced also the Noise2Void method. Hence, modifying the configuration files, an equivalent N2V network can be trained and tested.

### Training

First move to the project folder, then run:

```
# Train the model
python main.py [-c CONFIG_FILE_NAME] [-s RUN_NAME] [-r] [-g GPU_ID] [--thread THREAD_NUM]
```

The presented arguments handle the following functionalities:

- [-c CONFIG_FILE_NAME]:     Name of the configuration file (only the file name is required w/o extension ".yaml").
- [-s RUN_NAME]:             Name of the training session (optional but suggested).
- [-g GPU_ID]:               Gpu id number to use, only 1 gpu supported.
- [-r]:                      Flag to resume a previous training session (ON: resume, OFF: new run).
- [--thread THREAD_NUM]:     Number of thread for the dataloader (optional, default=4).

Further details of the model can be controlled directly from the configuration files (e.g. loss function, #epochs, batch_size, learning rate, ...).  
In folder `configs` I've left 4 examples you can use to write your configuration files. You can use them whether you want to run a NL-N2V or a N2V model.

Examples:

```
# Train NL-N2V on the SIDD dataset using gpu:0
python main.py -c sidd_train -s nl_n2v_sidd_00 -g 0

# Resume training session "nl_n2v_dnd_00" of NL-N2V on the DND dataset using gpu:1
python main.py -c dnd_train -s nl_n2v_dnd_00 -g 1 -r
```

**Note**:  
As can be seen in `main.py`, for comfort I've coded the arguments in a list before calling the main function. Hence, another possible solution is to change these values directly on the file, saving it and run: 

```
python main.py
```

---

### Testing

First move to the project folder, then run:

```
# Test the model
python test.py [-c CONFIG_FILE_NAME] [-s SESSION_NAME] [-e CHECKPOINT_EPOCH] [-g GPU_ID] [--thread THREAD_NUM] [--pretrained CHECKPOINT] [--test_img IMAGE_PATH] [--test_dir DIRECTORY_PATH]
```

The presented arguments handle the following functionalities:

- [-c CONFIG_FILE_NAME]:     Name of the configuration file (only the file name is required w/o extension ".yaml").
- [-s RUN_NAME]:             Name of the trained session (optional but suggested).
- [-e CHECKPOINT_EPOCH]:     Epoch number of the checkpoint (disabled when --pretrained is on).
- [-g GPU_ID]:               Gpu id number to use, only 1 gpu supported.
- [--thread THREAD_NUM]:     Number of thread for the dataloader (optional, default=4).
- [--pretrained CHECKPOINT]: Checkpoint of the model to test (name + extension)
- [--test_img IMAGE_PATH]:   Image to test the model on (optional, not tested thoroughly)
- [--test_dir DIRECTORY_PATH]: Directory of images to be tested (optional, not tested thoroughly) 

Test configuration scan be controlled from the configuration files.  
As for the training settings, in the `configs` folder you can find examples of test configurations.  
As you can see from the arguments, the method can be tested on 3 types of data: image, directory of images and dataset. The recommended (and correctly tested) way to do it is to set the path to the testing dataset in the configuration file.

Examples:

```
# Test pre-trained model on SIDD Benchmark in gpu:0
python test.py -c sidd_train -s nl_n2v_sidd_00 -g 0 --pretrained sidd_train.pth

# Test pre-trained model on an image in gpu:1
python test.py -c sidd_train -s nl_n2v_sidd_00 -g 1 --pretrained sidd_train.pth --test_img ./sample.png

# Test pre-trained model on an directory in gpu:0
python test.py -c sidd_train -s nl_n2v_sidd_00 -g 0 --pretrained sidd_train.pth --test_dir path/to/test/dir
```

**Note**:  
Also in the `test.py` file I've coded the arguments in a list. Hence, the tests can be run by changing the arguments directly on the list, saving the file and running: 

```
python test.py
```

## üìä Results

Here we report some quantitative and qualitative results obtained with NL-N2V.  
Please refer to the paper for more details.

![Quantitative comaprison table](/readme_figs/quantitative_results.PNG)

![Qualitative comparison figure](/readme_figs/qualitative_comparison.PNG)

### üß™ Reproducing experiments

From the links below you can download pre-trained checkpoints of our method.  

| Method | Dataset | Config file | Trained ckpt |
|--------|---------|-------------|--------------|
| NL-N2V | SIDD    | sidd_train.yaml     | [nl_n2v_sidd](https://polimi365-my.sharepoint.com/:f:/g/personal/10660793_polimi_it/EjLTcbdGfnlDoCD8_kpvJBkBV4XYH3XOZLN_05AqlhW2Iw?e=kKlV0n) |
| N2V    | SIDD    | n2v_sidd_train.yaml | [n2v_sidd](https://polimi365-my.sharepoint.com/:f:/g/personal/10660793_polimi_it/EjLTcbdGfnlDoCD8_kpvJBkBV4XYH3XOZLN_05AqlhW2Iw?e=kKlV0n) |
| NL-N2V | DND     | dnd_train.yaml      | [nl_n2v_dnd](https://polimi365-my.sharepoint.com/:f:/g/personal/10660793_polimi_it/EjLTcbdGfnlDoCD8_kpvJBkBV4XYH3XOZLN_05AqlhW2Iw?e=kKlV0n) |
| N2V    | DND     | n2v_dnd_train.yaml  | [See notes](https://polimi365-my.sharepoint.com/:f:/g/personal/10660793_polimi_it/EjLTcbdGfnlDoCD8_kpvJBkBV4XYH3XOZLN_05AqlhW2Iw?e=kKlV0n) |


After downloading the checkpoints, put them in the output folder at: `./output/run_name/checkpoint/`.  
Then run the following line: 

```
python test.py -c corresponding_run_conf -s run_name -g 0 --pretrained checkpoint.pth
```

As stated before, the suggested way to test these solutions is to provide the testing dataset through the configuration file.

**Note:**  
Unfortunately, I've lost the pre-trained checkpoints for the best NL-N2V and N2V models trained on DND. However, I still have the already bundled images I've sent to the official DND website for metric calculation. Moreover, I still have the weights of the NL-N2V model that acquired the performance in the middle row of the screenshot below.  
Hence, in the OneDrive folder, you can find the checkpoint of this model along with the already bundled images you can test yourself for reproducibility. Please refer to the paper if you want to find more details on the used parameters in order to re-train the models.

Here the screenshot of the DND submission page for proof:
![DND proof](/readme_figs/dnd_proof.PNG)

## üíæ Datasets (preparations & submissions)

NL-N2V has been tested on [SIDD](https://abdokamel.github.io/sidd/) and [DND](https://noise.visinf.tu-darmstadt.de/) datasets for spatially correlated noise.  
The results shown in the quantitative table has been calculated by the online testing tools of both datasets available at: 
- SIDD: (https://www.kaggle.com/competitions/sidd-benchmark-srgb-psnr)
- DND: (https://noise.visinf.tu-darmstadt.de/submit/)

Since the images contained in the datasets are too big to be used for training, I split them into sub images.  
In `prep_SIDD.py` and `prep_DND.py` I reported the code used to generate the datasets.

### SIDD

SIDD already provides the validation set in form of .mat files. Hence, the only preparation needed is for the training set.  

Running the `prep_SIDD.py` file splits the noisy images of SIDD into smaller patches of dimension 'crop_size' and overlapping of 'step' pixels. You can run the file using:

```
python prep_SIDD.py -c [CROP_SIZE] -s [STEP]
```

Arguments:
- [-c CROP_SIZE]: Size of the patch to crop (default = 512)
- [-s STEP]       Size of the overlapping portion between adjacent patches (default = 256)

**Note**:  
With the indicated default settings, the training set will be split in roughly 59k images (512x512x3).  
Be carefull to modify the file providing the correct path to the folder containing the original dataset and setting the correct path to the folder where you want to save the split training set. The suggested folder is: `./datasets/train/`.

### DND

DND doesn't provide a separate Validation set. However, they provide a way to construct the Test set which metrics are calculated through a submission to their website.  

The `prep_DND.py` file has been built to generate both the training and test sets. To create both sets, you need to run the file 2 times: one for the train set, one for the test set. Remember to comment out the function you **don't want** to run in the code.

You can run the file through:

```
python prep_DND.py -c [CROP_SIZE] -s [STEP] -df [PATH_TO_DATASET] -nf [PATH_TO_NEW_FOLDER]
```

Arguments:
- [-c CROP_SIZE]: Size of the path to crop (default = 512)
- [-s STEP]:      Size of the overlapping between adjacent patches (default = 256)
- [-df PATH_TO_DATASET]:    Path to the folder containing the original DND dataset
- [-nf PATH_TO_NEW_FOLDER]: Path to the folder that will contain the new dataset 

**Note:**  
Another way to run the code would be to prepare the arguments to pass to both functions in the `if __name__ == '__main__'` statement. Then, directly call the methods passing these lists to both of them. Finally, run:

```
python prep_DND.py
```

---

### Submissions

To asses the performance of the method on both datasets benchmark you must perform a submission to their respective websites.  
The files `prep_submission_SIDD.py` and `prep_submission_DND.py` help you prepare the correct format for both submissions.

**SIDD:**

SIDD submission requires to prepare a `.csv` file containing the denoised test images. The code in the `prep_submission_SIDD.py` helps you do that automatically.  
Before running the code, you need to: 
  - Modify the path to the folder containing the test images
  - Modify the path to the folder containing the **denoised** test images
  - Modify the name of the output `.csv` file 

Then run the code using:

```
python prep_submission_SIDD.py
```

**DND:**

DND submission requires to save into `.mat` files the denoised test images. The code in `prep_submission_DND.py` helps you do that automatically.  
Before running the code, you need to:
  - Modify the path to the folder containing the **denoised** test images
  - Modify the path to the folder where you want to store the `.mat` files to submit

Then run the code using:

```
python prep_submission_DND.py
```

## üìù Citation

Please cite this works as:

#TODO: Insert correct citation when the article is published


